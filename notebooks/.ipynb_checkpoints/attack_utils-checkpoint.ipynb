{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a643b291",
   "metadata": {},
   "source": [
    "# Attack Utils\n",
    "\n",
    "This notebook is a helper notebook containing functions required to attack the neural network models in the library. To see the variants of popular adversarial attacks that we developed to attack siamese neural networks, see 'attack_variants.ipynb'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c309014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"imports.ipynb\"\n",
    "%run \"helper_utils.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb6468",
   "metadata": {},
   "source": [
    "### Attacking standard convolutional and feed-forward neural networks\n",
    "\n",
    "The functions in the cell below are used to attack the standard, undefended models and their traditionally adversarially trained counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f514fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attacking standard convolutional and feed-forward neural networks\"\"\"\n",
    "\n",
    "def attack_model(model,original_weights, x, y, defended_weights='', eps=0.2, iter_size=0.05, num_iters=4, lp_norm=np.inf):\n",
    "    \"\"\"\n",
    "    Run this function to attack a CNN model with FGSM, BIM, PGD and MIM, and receive and evaluation. \n",
    "    If the weights for defended versions of the model are included, then the defended versions will \n",
    "    also be attack and evaluated.\n",
    "    params:\n",
    "        Tensorflow model: model\n",
    "        string: original_weights. The path to the weights of the undefended model.\n",
    "        np_array: x. Clean x data e.g. MNIST images.\n",
    "        np_array: y. Clean y labels.\n",
    "        string: defended_weights. Include a path to alternative weights for the model to see them evaluated.\n",
    "        float: eps. Attack epsilon (greater epsilon means more likely to produce strong adversarial examples, but less subtle)\n",
    "        float: iter_size. How much a perturbation will change in each iteration of the iterative attack\n",
    "        int: num_iters. The number of iterations/steps to the attacks\n",
    "        lp_norm. The lp norm can be 1, 2 or inf\n",
    "    Returns:\n",
    "        list: evals. The list of model evaluations (loss and accuracy)\n",
    "        list: accs. List of just accuracy scores.\n",
    "        \"\"\"\n",
    "    \n",
    "    # load the undefended weights and generate adversarial datasets to attack the undefended model\n",
    "    model.load_weights(original_weights)\n",
    "    x_fgsm = fast_gradient_method(model, x, eps, lp_norm)\n",
    "    x_bim = basic_iterative_method(model, x, eps, iter_size, num_iters, lp_norm)\n",
    "    x_madry = madry_et_al(model, x, eps, iter_size, num_iters, lp_norm)\n",
    "    x_mim = momentum_iterative_method(model, x, eps)\n",
    "    \n",
    "    evals = []\n",
    "    accs = []\n",
    "    \n",
    "    # if a second set of model weights is provided to defend the model, load them and evaluate their performance\n",
    "    # otherwise, evaluate the original model's performance against the adversarial datasets\n",
    "    if defended_weights !='':\n",
    "        model.load_weights(defended_weights)\n",
    "    \n",
    "    fgsm_eval = model.evaluate(x_fgsm,y)\n",
    "    bim_eval = model.evaluate(x_bim, y)\n",
    "    madry_eval = model.evaluate(x_madry,y)\n",
    "    mim_eval = model.evaluate(x_mim, y)\n",
    "    \n",
    "    evals.append(fgsm_eval)\n",
    "    evals.append(bim_eval)\n",
    "    evals.append(madry_eval)\n",
    "    evals.append(mim_eval)\n",
    "    \n",
    "    accs.append(fgsm_eval[1])\n",
    "    accs.append(bim_eval[1])\n",
    "    accs.append(madry_eval[1])\n",
    "    accs.append(mim_eval[1])\n",
    "    \n",
    "    return evals, accs\n",
    "\n",
    "def attack_model_variations(model,original_weights,model_weights,x,y,eps=0.2):\n",
    "    \"\"\"\n",
    "    Attack a standard CNN model and variations of that model with different weights.\n",
    "    \n",
    "    Params:\n",
    "        Tensorflow model: model. The model architecture.\n",
    "        string: original_weights. A path to the model's weights before any adversarial training or defences.\n",
    "        list: model_weights. A list of strings containing paths to the model's defended weights.\n",
    "        np_array: x. The test data that will be used to generate adversarial datasets.\n",
    "        np_array: y. The labels corresponding to the test data.\n",
    "        float: eps. The attack epsilon value. This dictates how much each pixel's value can change by in an adversarial attack.\n",
    "            0.2 is considered a large epsilon on normalised data, thus producing powerful attacks.\n",
    "            \n",
    "    Returns:\n",
    "        list: evals. A list of the evaluation scores for the models against the attacks.\n",
    "        list: accs. A list of the accuracy of each model variation against each adversarial attack.\n",
    "    \"\"\"\n",
    "    evals = []\n",
    "    accs = []\n",
    "    # run the attacks on every single variant of the model's weights\n",
    "    for weights in model_weights:\n",
    "        print(weights)\n",
    "        model.load_weights(weights)\n",
    "        new_evals, new_accs = attack_model(model, original_weights,x,y,defended_weights=weights,eps=0.2,iter_size=0.05,num_iters=4,lp_norm=np.inf)\n",
    "        evals.append(new_evals)\n",
    "        accs.append(new_accs)\n",
    "    return evals, accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7bc3c",
   "metadata": {},
   "source": [
    "### Attacking siamese neural networks\n",
    "\n",
    "The following functions are designed to attack siamese neural networks. The first uses our specially developed attack variants to specifically attack a siamese neural network's image pairs, and the second of which attacks the siamese verification networks with adversarial examples generated for the undefended standard neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006f1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_model_siamese(model, x, y, eps=0.2, iter_size=0.05, num_iters=4, lp_norm=np.inf, loss_fn=my_contrastive_loss, multi=True):\n",
    "    \"\"\"\n",
    "    Run each attack on a given siamese model, and return the evalutation metrics from each attack.\n",
    "    \n",
    "    Params:\n",
    "        Tensorflow model: model.\n",
    "        np_array: x.\n",
    "        np_array: y.\n",
    "        float: eps. Attack epsilon (greater epsilon means more likely to produce strong adversarial examples, but less subtle)\n",
    "        float: iter_size. How much a perturbation will change in each iteration of the iterative attack\n",
    "        int: num_iters. The number of iterations/steps to the attacks\n",
    "        lp_norm. The lp norm can be 1, 2 or inf\n",
    "        Loss function: loss_fn. Set to contrastive_loss as default as that is what our siamese models use. Only change if you\n",
    "            use a different loss function.\n",
    "        bool: multi. Set to true to apply perturbations to both images in the image pair, and false to apply only to a single\n",
    "            image. Multi= true produces stronger attacks.\n",
    "    Returns:\n",
    "        list: evals. The list of model evaluations (loss and accuracy)\n",
    "        list: accs. List of just accuracy scores. \n",
    "    \"\"\"\n",
    "    \n",
    "    # generate adversarial examples for each attack\n",
    "    x_fgsm = fgsm_siamese(model, x, eps, lp_norm, loss_fn=loss_fn, y=y, multi=True)\n",
    "    x_fgsm = process_adversarial_output(x_fgsm)\n",
    "    x_bim = pgd_siamese(model, x, eps,iter_size,num_iters, lp_norm, loss_fn=loss_fn, y=y,rand_init=0, multi=True)\n",
    "    x_bim = process_adversarial_output(x_bim)\n",
    "    x_pgd = pgd_siamese(model, x, eps, iter_size, num_iters, lp_norm, loss_fn=loss_fn, y=y, multi=True)\n",
    "    x_pgd = process_adversarial_output(x_pgd)\n",
    "    x_mim = mim_siamese(model, x, eps, loss_fn=loss_fn, y=y, multi=True)\n",
    "    x_mim = process_adversarial_output(x_mim)\n",
    "    \n",
    "    evals = []\n",
    "    accs = []\n",
    "    \n",
    "    # evaluate the model against each attack\n",
    "    fgsm_eval = siamese_model_evaluate(model, x_fgsm, y)\n",
    "    evals.append(fgsm_eval)\n",
    "    accs.append(fgsm_eval[1])\n",
    "    bim_eval = siamese_model_evaluate(model, x_bim, y)\n",
    "    evals.append(bim_eval)\n",
    "    accs.append(bim_eval[1])\n",
    "    pgd_eval = siamese_model_evaluate(model, x_pgd, y)\n",
    "    evals.append(pgd_eval)\n",
    "    accs.append(pgd_eval[1])\n",
    "    mim_eval = siamese_model_evaluate(model, x_mim, y)\n",
    "    accs.append(mim_eval[1])\n",
    "    evals.append(mim_eval)\n",
    "    return evals, accs\n",
    "\n",
    "def attack_siamese_models(model_weights,datasets,attacks=['FGSM','BIM','PGD','MIM'],threshold=0.5):\n",
    "    \"\"\"\n",
    "    Use this function to attack multiple variations of a siamese neural network model based on our architecture.\n",
    "    list: model_weights. A list of the different weights of the model that is being evaluated.\n",
    "    list: datasets. A list of the adversarial datasets to evaluate the siamese models against.\n",
    "    list: attacks. List of the attacks used. Leave as default unless additional attacks are added.\n",
    "    int: threshold. The threshold for the siamese similarity metric. If the model's output is lower than the threshold,\n",
    "        it predicts a true match. We find that 0.4 and 0.5 work best.\n",
    "    Returns:\n",
    "        list: siamese_evals. A list of the evaluation scores for the models against the attacks.\n",
    "        list: siamese_accs. A list of the accuracy of each model variation against each adversarial attack.\n",
    "    \"\"\"\n",
    "    model_architecture = get_siamese_model_architecture(np.asarray(datasets[0][0][0][0]).shape, embedding_dim=128,conv_size=32,audio=False, kernel_size=3)\n",
    "    siamese_evals = []\n",
    "    siamese_accs = []\n",
    "    for weights in model_weights:\n",
    "        print(weights)\n",
    "        model_architecture.load_weights(str(weights))\n",
    "        print('\\nModel: ' +str(weights))\n",
    "        for i in range(0,4):\n",
    "            accuracy, precision, auc, loss = siamese_model_evaluate(model_architecture,datasets[i][0],datasets[i][1], threshold=threshold)\n",
    "            print(attacks[i],': Accuracy ',accuracy,'Precision ',precision,'AUC ',auc,' Loss ',loss)\n",
    "            siamese_evals.append([loss,accuracy])\n",
    "            siamese_accs.append(accuracy)\n",
    "            print(accuracy)\n",
    "    return siamese_evals, siamese_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23e1dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_adversarial_output(adv_x):\n",
    "    \"\"\"\n",
    "    After running our siamese attack variants, we have to transpose the data to return it to its original dimensions\n",
    "    so that it is still accepted by the model it is intended for.\n",
    "    \n",
    "    Params:\n",
    "        list/np_array: adv_x. A list or numpy array of adversarial examples.\n",
    "    Returns:\n",
    "        np_array: adv_x. Return the transposed np_array.\n",
    "    \"\"\"\n",
    "    adv_x = adv_x.numpy()\n",
    "    adv_x = np.transpose(adv_x, (1,0,2,3,4))\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea9f6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attack_eval(model_eval, attacks = ['FGSM','BIM','PGD','MIM']):\n",
    "    \"\"\"\n",
    "    A clean way to print the evaluation scores of standard CNN models.\n",
    "    \n",
    "    Params:\n",
    "        List of tuples: model_eval. A list containing tuples for loss and accuracy, generated from a model's evaluate function.\n",
    "        list: attacks. A list of the attacks that the model is evaluated against. Only change from default if new attacks are\n",
    "            added.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for attack in attacks:\n",
    "        print(\"Model on \",attack,\" adversarial examples - Accuracy: \",model_eval[i][1],\" -  Loss: \",model_eval[i][0])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214515b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
