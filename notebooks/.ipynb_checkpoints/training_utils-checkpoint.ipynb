{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81708443",
   "metadata": {},
   "source": [
    "# Training Utils\n",
    "\n",
    "This notebook provides functions that help to build, train, and adversarially train standard neural network models and siamese neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf4b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"imports.ipynb\"\n",
    "%run \"helper_utils.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee278962",
   "metadata": {},
   "source": [
    "## Generating the Siamese Verification Network's Architecture\n",
    "\n",
    "The functions in the cell below show how we build our siamese adversarial verification networks and generate the architecture that we discuss in our report and use to defend against adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c4b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(input_shape, embedding_dim=128, conv_size=32,kernel_size=3):\n",
    "    \"\"\"\n",
    "    This function generates a feature extractor convolutional neural network for the siamese model, which uses two identical\n",
    "    feature extractor networks that identify features in the inputs.\n",
    "    \n",
    "    Params:\n",
    "        tuple of ints: input_shape. The shape of expected input data.\n",
    "        int: embedding_dim. The number of neurons in the final densely connected layer of the feature extractor.\n",
    "        int: conv_size. The number of filters to capture features in the convolutional layers. This is doubled in the\n",
    "            second convolutional layer.\n",
    "        int: kernel_size. The size of the kernel for learning features of input data.\n",
    "    Returns:\n",
    "        Tensorflow model: model. Return the feature extractor CNN model\n",
    "        \n",
    "    \"\"\"\n",
    "    # specify the inputs for the feature extractor network\n",
    "    inputs = Input(input_shape)\n",
    "    # define the first set of CONV => RELU => POOL => DROPOUT layers\n",
    "    x = Conv2D(conv_size, (kernel_size, kernel_size), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = MaxPooling2D(pool_size=(kernel_size, kernel_size))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # second set of CONV => RELU => POOL => DROPOUT layers\n",
    "    x = Conv2D(conv_size*2, (kernel_size, kernel_size), padding=\"same\", activation=\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    # prepare the final outputs\n",
    "    pooledOutput = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(embedding_dim)(pooledOutput)\n",
    "    # build the model\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_siamese_model_architecture(shape, embedding_dim=128, conv_size=32,audio=False, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Gets the model architecture for a siamese verification network for a given input shape.\n",
    "    \n",
    "    Params:\n",
    "        tuple of ints: input_shape. The shape of expected input data.\n",
    "        int: embedding_dim. The number of neurons in the final densely connected layer of the feature extractor.\n",
    "        int: conv_size. The number of filters to capture features in the convolutional layers. This is doubled in the\n",
    "            second convolutional layer.\n",
    "        bool: audio. Legacy bool that will be removed in future versions. Makes no difference whether true or false.\n",
    "        int: kernel_size. The size of the kernel for learning features of input data.\n",
    "    \"\"\"\n",
    "    img_a = Input(shape=shape)\n",
    "    img_b = Input(shape=shape)\n",
    "    # create a feature extractor CNN for both sides of the input pair\n",
    "    if audio:\n",
    "        feature_extractor = build_siamese_model(shape, embedding_dim=embedding_dim, conv_size=conv_size, kernel_size=kernel_size)\n",
    "    else:    \n",
    "        feature_extractor = build_siamese_model(shape, embedding_dim=embedding_dim, conv_size=conv_size, kernel_size=kernel_size)\n",
    "    feats_a = feature_extractor(img_a)\n",
    "    feats_b = feature_extractor(img_b)\n",
    "    # finally, construct the siamese network\n",
    "    distance = Lambda(euclidean_distance)([feats_a, feats_b])\n",
    "    model = Model(inputs=[img_a, img_b], outputs=distance)\n",
    "    model.compile(loss=contrastive_loss, optimizer=\"adam\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_siamese_model(model,x_train,y_train,x_test,y_test,batch_size=64,epochs=10):\n",
    "    \"\"\"\n",
    "    A function to simplify the process of training a siamese model.\n",
    "    \n",
    "    Params:\n",
    "        tensorflow model: model. The model to be trained.\n",
    "        np_array: x_train. Training dataset.\n",
    "        np_array: y_train. Training labels.\n",
    "        np_array: x_test. Test dataset.\n",
    "        np_array: y_test. Test labels.\n",
    "        int: batch_size. The number of data points in each training batch.\n",
    "        int: epochs. How many times the model will train on the full training set.\n",
    "        \n",
    "    Returns:\n",
    "        tensorflow model: model. The trained model.\n",
    "        callback object: siamese_history. Contains the training history of the model.\n",
    "    \"\"\"\n",
    "    siamese_history = model.fit(\n",
    "        [x_train[:, 0], x_train[:, 1]], y_train[:],\n",
    "        validation_data=([x_test[:, 0], x_test[:, 1]], y_test[:]),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, callbacks=callback_early_stop_reduceLROnPlateau)\n",
    "    return model, siamese_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4794aa",
   "metadata": {},
   "source": [
    "## Adversarial Training for standard neural networks and siamese neural networks\n",
    "\n",
    "The following functions adversarially train our neural network models to improve robustness to adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50556b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train_models(model,weights_path,datasets, y,level, attacks=['fgsm','bim','pgd','mim'],batch_size=32,epochs=15):\n",
    "    \"\"\"\n",
    "    Adversarially train a standard neural network model, creating four new models, one for each of our attacks.\n",
    "    The naming convention for new model weights is to add the attack type and level of training e.g. 1 for one-shot, to\n",
    "    the end of the original model_weights path. For instance, MNIST_weights_fgsm_1.h5. The models are saved to the 'models'\n",
    "    section of the library.\n",
    "    \n",
    "    Params:\n",
    "        tensorflow model: model. The model to be trained.\n",
    "        string: weights_path. The path to the weights for the model.\n",
    "        list: datasets. The adversarial datasets to be trained on.\n",
    "        np_array: y. The dataset labels.\n",
    "        int: level. The level of adversarial training, with 1 indicating one-shot adversarial training.\n",
    "        list: attacks. The attacks that the model will be trained on. Only change from default if new attacks are added.\n",
    "        int: batch_size. The number of data points in each training batch.\n",
    "        int: epochs. The number of times the model will train on the full dataset.\n",
    "    Returns:\n",
    "        list: paths. A list of strings containing the paths to the defended model weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # we incorporate early stopping to prevent overfitting\n",
    "    paths = []\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='loss',verbose=1, patience=2)\n",
    "\n",
    "    # generate an adversarially trained model for each adversarial dataset and save each model when trained\n",
    "    if len(datasets) == 4:\n",
    "        print('fgsm model')\n",
    "        model.load_weights(weights_path) # load undefended weights each time before adversarial training\n",
    "        model.fit(datasets[0],y,epochs=epochs,batch_size=batch_size, callbacks=callbacks)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[0])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path) # save the new weights\n",
    "        paths.append(path)\n",
    "        \n",
    "        print('bim model')\n",
    "        model.load_weights(weights_path)\n",
    "        model.fit(datasets[1],y,epochs=epochs,batch_size=batch_size, callbacks=callbacks)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[1])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "        \n",
    "        print('pgd model')\n",
    "        model.load_weights(weights_path)\n",
    "        model.fit(datasets[2],y,epochs=epochs,batch_size=batch_size, callbacks=callbacks)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[2])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "        \n",
    "        print('mim model')\n",
    "        model.load_weights(weights_path)\n",
    "        model.fit(datasets[3],y,epochs=epochs,batch_size=batch_size, callbacks=callbacks)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[3])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "    else:\n",
    "        print('Datasets must be an array of 4 datasets')\n",
    "        \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f3a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train_siamese_models(model,weights_path,train_datasets, test_datasets,level=1, attacks=['fgsm','bim','pgd','mim'],batch_size=32,epochs=100):\n",
    "    \"\"\"\n",
    "    Adversarially train a siamese neural network model, creating four new models, one for each of our attacks.\n",
    "    The naming convention for new model weights is to add the attack type and level of training e.g. 1 for one-shot, to\n",
    "    the end of the original model_weights path. For instance, MNIST_weights_fgsm_1.h5. The models are saved to the 'models'\n",
    "    section of the library.\n",
    "    \n",
    "    Params:\n",
    "        tensorflow model: model. The model to be trained.\n",
    "        string: weights_path. The path to the weights for the model.\n",
    "        list: train_datasets. The adversarial datasets to be trained on.\n",
    "        list: test_datasets. These are used for training validation.\n",
    "        int: level. The level of adversarial training. The value is no longer required here and will be removed in a\n",
    "            future version.\n",
    "        list: attacks. The attacks that the model will be trained on. Only change from default if new attacks are added.\n",
    "        int: batch_size. The number of data points in each training batch.\n",
    "        int: epochs. The number of times the model will train on the full dataset if early stopping does not occur.\n",
    "    Returns:\n",
    "    list: paths. List of strings containing paths to the defended model weights.\n",
    "    list: histories. List of training histories for each model.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    histories = []\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='loss',verbose=1, patience=2)\n",
    "\n",
    "    if len(train_datasets) == 4:\n",
    "        print('fgsm model')\n",
    "        model.load_weights(weights_path)\n",
    "        model, history = train_siamese_model(model,train_datasets[0][0],train_datasets[0][1],test_datasets[0][0],test_datasets[0][1], batch_size=batch_size,epochs=epochs)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[0])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "        histories.append(history)\n",
    "        \n",
    "        print('bim model')\n",
    "        model.load_weights(weights_path)\n",
    "        model, history = train_siamese_model(model,train_datasets[1][0],train_datasets[1][1],test_datasets[1][0],test_datasets[1][1], batch_size=batch_size,epochs=epochs)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[1])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "        histories.append(history)\n",
    "        \n",
    "        print('pgd model')\n",
    "        model.load_weights(weights_path)\n",
    "        model, history = train_siamese_model(model,train_datasets[2][0],train_datasets[2][1],test_datasets[2][0],test_datasets[2][1], batch_size=batch_size,epochs=epochs)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[2])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "        histories.append(history)\n",
    "        \n",
    "        print('mim model')\n",
    "        model.load_weights(weights_path)\n",
    "        model, history = train_siamese_model(model,train_datasets[3][0],train_datasets[3][1],test_datasets[3][0],test_datasets[3][1], batch_size=batch_size,epochs=epochs)\n",
    "        path = str(weights_path[:-3])+'_'+str(attacks[3])+'_'+str(level)+'.h5'\n",
    "        model.save_weights(path)\n",
    "        paths.append(path)\n",
    "        histories.append(history)\n",
    "    else:\n",
    "        print('Datasets must be an array of 4 datasets')\n",
    "        \n",
    "    return paths, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511ddc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
